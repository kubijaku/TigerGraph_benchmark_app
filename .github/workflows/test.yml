name: TigerGraph Query Performance Test

on:
    workflow_dispatch:

jobs:
    performance-test:
        runs-on: ubuntu-latest

        steps:
          - name: Checkout
            uses: actions/checkout@v4
            with:
              lfs: true

          - name: Start TigerGraph container
            run: |
                docker run -d --name tigergraph \
                -p 14022:14022 \
                -p 9000:9000 \
                -v $PWD/initialization:/home/tigergraph/mydata \
                -v $PWD/queries:/home/tigergraph/queries \
                -v #PWD/test1:/home/tigergraph/test1 \
                tigergraph/community:4.2.2

          - name: Start TigerGraph services inside container
            run: |
                echo "Initializing TigerGraph with gadmin..."
                docker exec tigergraph bash -c \
                    "./tigergraph/app/cmd/gadmin start all"
        
          - name: Print head of cskg file
            run: |
                docker exec tigergraph bash -c \
                    "head -n 5 /home/tigergraph/mydata/cskg.tsv"
        

          - name: Run init script
            run: |
                docker exec tigergraph bash -c \
                    "source /home/tigergraph/.bashrc && ./tigergraph/app/cmd/gsql < /home/tigergraph/mydata/import.gsql"

          - name: Add query01 to TigerGraph
            run: |
                docker exec tigergraph bash -c \
                    "source /home/tigergraph/.bashrc && ./tigergraph/app/cmd/gsql < /home/tigergraph/queries/query_01.gsql"
                

          - name: Run query01 on sample-nodes.csv and collect metrics
            id: metrics
            run: |
                echo "Running query_01 for all IDs in sample-nodes.csv"

                set -euo pipefail

                START_TIME=$(date +%s)

                # Run all queries in a single measured block
                OUTPUT=$(docker exec tigergraph bash -lc "
                set -e
                tail -n +2 /home/tigergraph/test1/sample_nodes.csv | cut -d',' -f1 | \
                /usr/bin/time -v bash -c '
                    while read -r ID; do
                    gsql \"USE GRAPH ADS RUN QUERY query_01(\\\"$ID\\\")\" > /dev/null
                    done
                '
                " 2>&1)

                END_TIME=$(date +%s)
                TOTAL_TIME=$((END_TIME - START_TIME))

                # Extract peak memory (KB)
                PEAK_MEM=$(echo \"$OUTPUT\" | grep 'Maximum resident set size' | awk '{print \$6}')

                echo "Total execution time: $TOTAL_TIME seconds"
                echo "Peak memory usage  : $PEAK_MEM KB"

                echo "final_time=$TOTAL_TIME" >> $GITHUB_OUTPUT
                echo "final_memory=$PEAK_MEM" >> $GITHUB_OUTPUT


          - name: Display results
            run: |
                echo "Final measured time   : ${{ steps.metrics.outputs.final_time }}"
                echo "Final measured memory : ${{ steps.metrics.outputs.final_memory }} KB"
